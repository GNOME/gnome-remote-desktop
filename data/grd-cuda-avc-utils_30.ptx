//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_30
.address_size 64

	// .globl	rgb_to_y                // -- Begin function rgb_to_y
                                        // @rgb_to_y
.visible .func  (.param .b32 func_retval0) rgb_to_y(
	.param .b32 rgb_to_y_param_0,
	.param .b32 rgb_to_y_param_1,
	.param .b32 rgb_to_y_param_2
)
{
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<7>;

// %bb.0:
	ld.param.u8 	%rs1, [rgb_to_y_param_0];
	ld.param.u8 	%rs2, [rgb_to_y_param_1];
	mul.wide.u16 	%r1, %rs1, 54;
	ld.param.u8 	%rs3, [rgb_to_y_param_2];
	mul.wide.u16 	%r2, %rs2, 183;
	add.s32 	%r3, %r2, %r1;
	mul.wide.u16 	%r4, %rs3, 18;
	add.s32 	%r5, %r3, %r4;
	shr.u32 	%r6, %r5, 8;
	st.param.b32 	[func_retval0], %r6;
	ret;
                                        // -- End function
}
	// .globl	rgb_to_u                // -- Begin function rgb_to_u
.visible .func  (.param .b32 func_retval0) rgb_to_u(
	.param .b32 rgb_to_u_param_0,
	.param .b32 rgb_to_u_param_1,
	.param .b32 rgb_to_u_param_2
)                                       // @rgb_to_u
{
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<7>;

// %bb.0:
	ld.param.u8 	%rs1, [rgb_to_u_param_0];
	ld.param.u8 	%rs2, [rgb_to_u_param_1];
	mul.wide.u16 	%r1, %rs1, -29;
	ld.param.u8 	%rs3, [rgb_to_u_param_2];
	mul.wide.u16 	%r2, %rs2, -99;
	add.s32 	%r3, %r2, %r1;
	mul.wide.u16 	%r4, %rs3, 128;
	add.s32 	%r5, %r3, %r4;
	cvt.u16.u32 	%rs4, %r5;
	shr.u16 	%rs5, %rs4, 8;
	xor.b16  	%rs6, %rs5, 128;
	cvt.u32.u16 	%r6, %rs6;
	st.param.b32 	[func_retval0], %r6;
	ret;
                                        // -- End function
}
	// .globl	rgb_to_v                // -- Begin function rgb_to_v
.visible .func  (.param .b32 func_retval0) rgb_to_v(
	.param .b32 rgb_to_v_param_0,
	.param .b32 rgb_to_v_param_1,
	.param .b32 rgb_to_v_param_2
)                                       // @rgb_to_v
{
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<7>;

// %bb.0:
	ld.param.u8 	%rs1, [rgb_to_v_param_0];
	ld.param.u8 	%rs2, [rgb_to_v_param_1];
	mul.wide.u16 	%r1, %rs1, 128;
	ld.param.u8 	%rs3, [rgb_to_v_param_2];
	mul.wide.u16 	%r2, %rs2, -116;
	add.s32 	%r3, %r2, %r1;
	mul.wide.u16 	%r4, %rs3, -12;
	add.s32 	%r5, %r3, %r4;
	cvt.u16.u32 	%rs4, %r5;
	shr.u16 	%rs5, %rs4, 8;
	xor.b16  	%rs6, %rs5, 128;
	cvt.u32.u16 	%r6, %rs6;
	st.param.b32 	[func_retval0], %r6;
	ret;
                                        // -- End function
}
	// .globl	convert_2x2_bgrx_area_to_yuv420_nv12 // -- Begin function convert_2x2_bgrx_area_to_yuv420_nv12
.visible .entry convert_2x2_bgrx_area_to_yuv420_nv12(
	.param .u64 .ptr .align 1 convert_2x2_bgrx_area_to_yuv420_nv12_param_0,
	.param .u64 .ptr .align 1 convert_2x2_bgrx_area_to_yuv420_nv12_param_1,
	.param .u16 convert_2x2_bgrx_area_to_yuv420_nv12_param_2,
	.param .u16 convert_2x2_bgrx_area_to_yuv420_nv12_param_3,
	.param .u16 convert_2x2_bgrx_area_to_yuv420_nv12_param_4,
	.param .u16 convert_2x2_bgrx_area_to_yuv420_nv12_param_5,
	.param .u16 convert_2x2_bgrx_area_to_yuv420_nv12_param_6
)                                       // @convert_2x2_bgrx_area_to_yuv420_nv12
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<27>;
	.reg .b32 	%r<104>;
	.reg .b64 	%rd<25>;

// %bb.0:
	ld.param.u64 	%rd7, [convert_2x2_bgrx_area_to_yuv420_nv12_param_0];
	ld.param.u64 	%rd8, [convert_2x2_bgrx_area_to_yuv420_nv12_param_1];
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd2, %rd7;
	mov.u32 	%r32, %ctaid.x;
	ld.param.u16 	%rs12, [convert_2x2_bgrx_area_to_yuv420_nv12_param_4];
	mov.u32 	%r33, %ntid.x;
	mov.u32 	%r34, %tid.x;
	mad.lo.s32 	%r1, %r32, %r33, %r34;
	and.b32  	%r35, %r1, 65535;
	shr.u16 	%rs13, %rs12, 1;
	cvt.u32.u16 	%r36, %rs13;
	setp.ge.u32 	%p1, %r35, %r36;
	@%p1 bra 	$L__BB3_11;
// %bb.1:
	ld.param.u16 	%rs10, [convert_2x2_bgrx_area_to_yuv420_nv12_param_5];
	mov.u32 	%r37, %tid.y;
	mov.u32 	%r38, %ntid.y;
	mov.u32 	%r39, %ctaid.y;
	mad.lo.s32 	%r2, %r39, %r38, %r37;
	and.b32  	%r3, %r2, 65535;
	cvt.u32.u16 	%r4, %rs10;
	shr.u32 	%r40, %r4, 1;
	setp.ge.u32 	%p2, %r3, %r40;
	@%p2 bra 	$L__BB3_11;
// %bb.2:
	ld.param.u16 	%rs11, [convert_2x2_bgrx_area_to_yuv420_nv12_param_6];
	ld.param.u16 	%rs9, [convert_2x2_bgrx_area_to_yuv420_nv12_param_3];
	ld.param.u16 	%rs8, [convert_2x2_bgrx_area_to_yuv420_nv12_param_2];
	cvt.u32.u16 	%r5, %rs8;
	shl.b32 	%r42, %r1, 1;
	shl.b32 	%r43, %r2, 1;
	and.b32  	%r6, %r43, 65534;
	mul.lo.s32 	%r44, %r6, %r5;
	mul.wide.u32 	%rd9, %r44, 4;
	add.s64 	%rd10, %rd1, %rd9;
	and.b32  	%r7, %r42, 65534;
	cvt.u64.u32 	%rd11, %r7;
	mul.wide.u32 	%rd12, %r7, 4;
	add.s64 	%rd3, %rd10, %rd12;
	cvt.u32.u16 	%r45, %rs11;
	mul.lo.s32 	%r46, %r6, %r45;
	cvt.u64.u32 	%rd13, %r46;
	add.s64 	%rd14, %rd2, %rd13;
	add.s64 	%rd4, %rd14, %rd11;
	add.s32 	%r47, %r46, %r45;
	cvt.u64.u32 	%rd15, %r47;
	mul.lo.s32 	%r48, %r45, %r4;
	cvt.u64.u32 	%rd17, %r48;
	add.s64 	%rd18, %rd2, %rd17;
	mul.lo.s32 	%r49, %r3, %r45;
	cvt.u64.u32 	%rd19, %r49;
	setp.ge.u32 	%p3, %r7, %r5;
	cvt.u32.u16 	%r9, %rs9;
	setp.ge.u32 	%p4, %r6, %r9;
	mov.b32 	%r98, 0;
	mov.b16 	%rs25, 0;
	or.pred  	%p5, %p3, %p4;
	mov.u16 	%rs24, %rs25;
	mov.u32 	%r99, %r98;
	mov.u32 	%r100, %r98;
	@%p5 bra 	$L__BB3_4;
// %bb.3:
	ld.global.u32 	%r50, [%rd3];
	and.b32  	%r100, %r50, 255;
	bfe.u32 	%r99, %r50, 8, 8;
	bfe.u32 	%r98, %r50, 16, 8;
	mul.lo.s32 	%r51, %r100, 18;
	mad.lo.s32 	%r52, %r98, 54, %r51;
	mad.lo.s32 	%r53, %r99, 183, %r52;
	shr.u32 	%r54, %r53, 8;
	cvt.u16.u32 	%rs24, %r54;
$L__BB3_4:
	add.s64 	%rd16, %rd2, %rd15;
	add.s64 	%rd20, %rd18, %rd19;
	or.b32  	%r8, %r6, 1;
	st.global.u8 	[%rd4], %rs24;
	or.b32  	%r16, %r7, 1;
	setp.ge.u32 	%p7, %r16, %r5;
	or.pred  	%p8, %p7, %p4;
	@%p8 bra 	$L__BB3_6;
// %bb.5:
	ld.global.u32 	%r57, [%rd3+4];
	and.b32  	%r58, %r57, 255;
	add.s32 	%r100, %r58, %r100;
	bfe.u32 	%r59, %r57, 8, 8;
	add.s32 	%r99, %r59, %r99;
	bfe.u32 	%r60, %r57, 16, 8;
	add.s32 	%r98, %r60, %r98;
	mul.lo.s32 	%r61, %r58, 18;
	mad.lo.s32 	%r62, %r60, 54, %r61;
	mad.lo.s32 	%r63, %r59, 183, %r62;
	shr.u32 	%r64, %r63, 8;
	cvt.u16.u32 	%rs25, %r64;
$L__BB3_6:
	add.s64 	%rd5, %rd16, %rd11;
	add.s64 	%rd6, %rd20, %rd11;
	st.global.u8 	[%rd4+1], %rs25;
	setp.ge.u32 	%p10, %r8, %r9;
	or.pred  	%p11, %p3, %p10;
	@%p11 bra 	$L__BB3_9;
// %bb.7:
	mul.wide.u32 	%rd21, %r5, 4;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.u32 	%r68, [%rd22];
	and.b32  	%r69, %r68, 255;
	add.s32 	%r100, %r69, %r100;
	bfe.u32 	%r70, %r68, 8, 8;
	add.s32 	%r99, %r70, %r99;
	bfe.u32 	%r71, %r68, 16, 8;
	add.s32 	%r98, %r71, %r98;
	mul.lo.s32 	%r72, %r69, 18;
	mad.lo.s32 	%r73, %r71, 54, %r72;
	mad.lo.s32 	%r74, %r70, 183, %r73;
	shr.u32 	%r75, %r74, 8;
	st.global.u8 	[%rd5], %r75;
	mov.b16 	%rs26, 0;
	@%p7 bra 	$L__BB3_10;
// %bb.8:
	add.s16 	%rs1, %rs8, 1;
	cvt.u32.u16 	%r76, %rs1;
	mul.wide.u32 	%rd23, %r76, 4;
	add.s64 	%rd24, %rd3, %rd23;
	ld.global.u32 	%r77, [%rd24];
	and.b32  	%r78, %r77, 255;
	add.s32 	%r100, %r78, %r100;
	bfe.u32 	%r79, %r77, 8, 8;
	add.s32 	%r99, %r79, %r99;
	bfe.u32 	%r80, %r77, 16, 8;
	add.s32 	%r98, %r80, %r98;
	mul.lo.s32 	%r81, %r78, 18;
	mad.lo.s32 	%r82, %r80, 54, %r81;
	mad.lo.s32 	%r83, %r79, 183, %r82;
	shr.u32 	%r84, %r83, 8;
	cvt.u16.u32 	%rs26, %r84;
	bra.uni 	$L__BB3_10;
$L__BB3_9:
	mov.b16 	%rs26, 0;
	st.global.u8 	[%rd5], %rs26;
$L__BB3_10:
	st.global.u8 	[%rd5+1], %rs26;
	shr.u32 	%r85, %r100, 2;
	shr.u32 	%r86, %r99, 2;
	shr.u32 	%r87, %r98, 2;
	mul.lo.s32 	%r88, %r87, 65507;
	mad.lo.s32 	%r89, %r86, 65437, %r88;
	shl.b32 	%r90, %r85, 7;
	add.s32 	%r91, %r89, %r90;
	cvt.u16.u32 	%rs18, %r91;
	shr.u16 	%rs19, %rs18, 8;
	xor.b16  	%rs20, %rs19, 128;
	st.global.u8 	[%rd6], %rs20;
	shl.b32 	%r92, %r87, 7;
	mad.lo.s32 	%r93, %r86, 65420, %r92;
	mad.lo.s32 	%r94, %r85, 65524, %r93;
	cvt.u16.u32 	%rs21, %r94;
	shr.u16 	%rs22, %rs21, 8;
	xor.b16  	%rs23, %rs22, 128;
	st.global.u8 	[%rd6+1], %rs23;
$L__BB3_11:
	ret;
                                        // -- End function
}
